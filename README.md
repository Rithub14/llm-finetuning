ğŸ“˜ Customer Support LLM â€” Finetuned Gemma 2B (LoRA)

A lightweight, end-to-end AI assistant for customer support queries.
Built using FastAPI, Streamlit, LoRA-finetuned Gemma-2B, and SQLite logging.

ğŸš€ Overview

This project fine-tunes Google Gemma-2B-IT using LoRA on a customer-support dataset (Bitext).
It exposes a clean backend API and a simple chat UI for interacting with the model.

Key Features

ğŸ§  LoRA-finetuned Gemma-2B model for intent-based support responses

âš¡ FastAPI backend with clean modular structure

ğŸ’¬ Streamlit chat interface (runs automatically when backend starts)

ğŸ—„ SQLite database logging of all user queries & model responses

ğŸ”§ Modular codebase: services, routers, models, utils

ğŸ§¹ Production-style project structure + packaging

ğŸ— Architecture
+------------------+       +-----------------------------+
|   Streamlit UI   | <---> |        FastAPI API          |
+------------------+       +-------------+---------------+
                                      |
                                      v
                         +---------------------------+
                         |  LoRA-finetuned Gemma-2B  |
                         +---------------------------+
                                      |
                                      v
                             +----------------+
                             |   SQLite DB    |
                             +----------------+

ğŸ“¦ Project Structure
llm_finetuning/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/            # FastAPI routes
â”‚   â”œâ”€â”€ core/           # Streamlit launcher
â”‚   â”œâ”€â”€ db/             # SQLite + session
â”‚   â”œâ”€â”€ models/         # SQLModel ORM
â”‚   â”œâ”€â”€ services/       # Inference logic
â”‚   â”œâ”€â”€ utils/          # Prompt builder
â”‚   â””â”€â”€ main.py         # FastAPI entrypoint
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ app.py          # Streamlit UI
â”‚
â”œâ”€â”€ model/
â”‚   â””â”€â”€ adapters/       # LoRA checkpoint
â”‚
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ Makefile
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore

âš™ï¸ Installation
1ï¸âƒ£ Create environment
uv venv
source .venv/bin/activate

2ï¸âƒ£ Install dependencies
uv sync

3ï¸âƒ£ Start backend + UI
uvicorn app.main:app --reload


Streamlit automatically runs in background on port 8001.

Open browser:

http://localhost:8000/

ğŸ§ª Example API Call
curl -X POST "http://localhost:8000/generate/" \
     -H "Content-Type: application/json" \
     -d '{"query": "How do I track my order?"}'


Response:

{
  "response": "You can track your order by visiting..."
}

ğŸ—„ Database Logging

Every query is stored in SQLite:

user query

model response

timestamp

latency

model version

View manually:

sqlite3 app/db/queries.db
SELECT * FROM querylog;

ğŸ“š Model Fine-Tuning

The LoRA training was performed using:

transformers

peft

SFTTrainer (from TRL)

Kaggle â†’ Bitext Customer Support dataset

Output adapters are stored in:

model/adapters/checkpoint-939/

ğŸ“„ License

MIT License (add LICENSE file if needed).